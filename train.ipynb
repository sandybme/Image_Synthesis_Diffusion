{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sandhanakrishnan/anaconda3/envs/deepl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "# from torchinfo import summary\n",
    "\n",
    "from simple_diffusion.scheduler import DDIMScheduler\n",
    "from simple_diffusion.model import UNet\n",
    "from simple_diffusion.utils import save_images, normalize_to_neg_one_to_one\n",
    "from simple_diffusion.dataset import CustomDataset, get_dataset\n",
    "import pandas as pd\n",
    "import webdataset as wds\n",
    "\n",
    "from simple_diffusion.ema import EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = '/Users/Sandhanakrishnan/Image_Synthesis_Diffusion/Train'\n",
    "\n",
    "image_paths = [os.path.join(data_path, filename) for filename in os.listdir(data_path) if filename.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = pd.DataFrame(image_paths, columns=['image_path'])\n",
    "csv_file_path = 'train.csv'\n",
    "data_df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "n_timesteps = 1000\n",
    "n_inference_timesteps = 250\n",
    "\n",
    "def _grayscale_to_rgb(img):\n",
    "    if img.mode != \"RGB\":\n",
    "        return img.convert(\"RGB\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"mps\")\n",
    "    model = UNet(3, image_size=args.resolution, hidden_dims=[64, 128, 256, 512])\n",
    "    noise_scheduler = DDIMScheduler(num_train_timesteps=n_timesteps,\n",
    "                                    beta_schedule=\"cosine\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    if args.pretrained_model_path:\n",
    "        pretrained = torch.load(args.pretrained_model_path)[\"model_state\"]\n",
    "        model.load_state_dict(pretrained)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "    )\n",
    "\n",
    "    tfms = transforms.Compose([\n",
    "        transforms.Resize((args.resolution, args.resolution)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    # df = pd.read_pickle(args.dataset_path)\n",
    "    df = pd.read_csv(args.dataset_path)\n",
    "    dataset = CustomDataset(df)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "    steps_per_epcoch = len(train_dataloader)\n",
    "\n",
    "    total_num_steps = (steps_per_epcoch * args.num_epochs) // args.gradient_accumulation_steps\n",
    "    total_num_steps += int(total_num_steps * 10/100)\n",
    "    gamma = args.gamma\n",
    "    ema = EMA(model, gamma, total_num_steps)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps,\n",
    "        num_training_steps=total_num_steps,\n",
    "    )\n",
    "\n",
    "    # summary(model, [(1, 3, args.resolution, args.resolution), (1,)], verbose=1)\n",
    "\n",
    "    scaler = GradScaler(enabled=args.fp16_precision)\n",
    "    global_step = 0\n",
    "    losses = []\n",
    "    for epoch in range(args.num_epochs):\n",
    "        progress_bar = tqdm(total=steps_per_epcoch)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        losses_log = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # print(batch[\"image\"])\n",
    "            clean_images = batch[\"image\"].to(device)\n",
    "            clean_images = normalize_to_neg_one_to_one(clean_images)\n",
    "\n",
    "            batch_size = clean_images.shape[0]\n",
    "            noise = torch.randn(clean_images.shape).to(device)\n",
    "            timesteps = torch.randint(0,\n",
    "                                      noise_scheduler.num_train_timesteps,\n",
    "                                      (batch_size,),\n",
    "                                      device=device).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise,\n",
    "                                                     timesteps)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=args.fp16_precision):\n",
    "                noise_pred = model(noisy_images, timesteps)[\"sample\"]\n",
    "                loss = F.l1_loss(noise_pred, noise)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            ema.update_params(gamma)\n",
    "            gamma = ema.update_gamma(global_step)\n",
    "\n",
    "            if args.use_clip_grad:\n",
    "                clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            losses_log += loss.detach().item()\n",
    "            logs = {\n",
    "                \"loss_avg\": losses_log / (step + 1),\n",
    "                \"loss\": loss.detach().item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                \"step\": global_step,\n",
    "                \"gamma\": gamma\n",
    "            }\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "            # Generate sample images for visual inspection\n",
    "            if global_step % args.save_model_steps == 0:\n",
    "                ema.ema_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # has to be instantiated every time, because of reproducibility\n",
    "                    generator = torch.manual_seed(0)\n",
    "                    generated_images = noise_scheduler.generate(\n",
    "                        ema.ema_model,\n",
    "                        num_inference_steps=n_inference_timesteps,\n",
    "                        generator=generator,\n",
    "                        eta=1.0,\n",
    "                        use_clipped_model_output=True,\n",
    "                        batch_size=args.eval_batch_size,\n",
    "                        output_type=\"numpy\")\n",
    "\n",
    "                    save_images(generated_images, epoch, args)\n",
    "\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            'model_state': model.state_dict(),\n",
    "                            'ema_model_state': ema.ema_model.state_dict(),\n",
    "                            'optimizer_state': optimizer.state_dict(),\n",
    "                        }, args.output_dir)\n",
    "\n",
    "        progress_bar.close()\n",
    "        losses.append(losses_log / (step + 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulationArgs:\n",
    "    def __init__(self,\n",
    "                 dataset_name='',\n",
    "                 dataset_path='/Users/Sandhanakrishnan/Image_Synthesis_Diffusion/train.csv',\n",
    "                 dataset_config_name=None,\n",
    "                 output_dir=\"trained_models/ddpm-model-64.pth\",\n",
    "                 samples_dir=\"test_samples/\",\n",
    "                 loss_logs_dir=\"training_logs\",\n",
    "                 cache_dir=None,\n",
    "                 resolution=64,\n",
    "                 train_batch_size=1,\n",
    "                 eval_batch_size=1,\n",
    "                 num_epochs=1,\n",
    "                 save_model_steps=100,\n",
    "                 gradient_accumulation_steps=1,\n",
    "                 learning_rate=1e-4,\n",
    "                 lr_scheduler=\"cosine\",\n",
    "                 lr_warmup_steps=100,\n",
    "                 adam_beta1=0.9,\n",
    "                 adam_beta2=0.99,\n",
    "                 adam_weight_decay=0.0,\n",
    "                 use_clip_grad=False,\n",
    "                 logging_dir=\"logs\",\n",
    "                 pretrained_model_path=None,\n",
    "                 fp16_precision=False,\n",
    "                 gamma=0.996):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_config_name = dataset_config_name\n",
    "        self.output_dir = output_dir\n",
    "        self.samples_dir = samples_dir\n",
    "        self.loss_logs_dir = loss_logs_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.resolution = resolution\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.save_model_steps = save_model_steps\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.lr_warmup_steps = lr_warmup_steps\n",
    "        self.adam_beta1 = adam_beta1\n",
    "        self.adam_beta2 = adam_beta2\n",
    "        self.adam_weight_decay = adam_weight_decay\n",
    "        self.use_clip_grad = use_clip_grad\n",
    "        self.logging_dir = logging_dir\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.fp16_precision = fp16_precision\n",
    "        self.gamma = gamma\n",
    "\n",
    "    # Example validation method\n",
    "    def validate_args(self):\n",
    "        if self.dataset_name is None and self.dataset_path is None:\n",
    "            raise ValueError(\"You must specify either a dataset name or a dataset path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yfcc7m\n",
      "0.0002\n"
     ]
    }
   ],
   "source": [
    "args = SimulationArgs(dataset_name=\"yfcc7m\", train_batch_size=32, learning_rate=2e-4)\n",
    "\n",
    "# Access arguments like so\n",
    "print(args.dataset_name)\n",
    "print(args.learning_rate)\n",
    "\n",
    "# Don't forget to validate your arguments if needed\n",
    "args.validate_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s]/Users/Sandhanakrishnan/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1702400234613/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "Epoch 0:  10%|█         | 1/10 [01:40<15:07, 100.86s/it, gamma=0.996, loss=0.852, loss_avg=0.852, lr=2e-6, step=0]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mfp16_precision):\n\u001b[0;32m---> 73\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     74\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(noise_pred, noise)\n\u001b[1;32m     76\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Image_Synthesis_Diffusion/simple_diffusion/model/unet.py:193\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, sample, timesteps)\u001b[0m\n\u001b[1;32m    191\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block1(x, t_emb)\n\u001b[1;32m    192\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_attn(x)\n\u001b[0;32m--> 193\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid_block2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block1, block2, attn, upsample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks:\n\u001b[1;32m    196\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, skips\u001b[38;5;241m.\u001b[39mpop()), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Image_Synthesis_Diffusion/simple_diffusion/model/unet.py:86\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x, temb)\u001b[0m\n\u001b[1;32m     83\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_conv(x)\n\u001b[1;32m     85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m---> 86\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinearity(x)\n\u001b[1;32m     89\u001b[0m temb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_emb_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinearity(temb))\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/nn/modules/normalization.py:279\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/nn/functional.py:2558\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2557\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepl/lib/python3.10/site-packages/torch/_refs/__init__.py:3032\u001b[0m, in \u001b[0;36mnative_group_norm\u001b[0;34m(input, weight, bias, batch_size, num_channels, flattened_inner_size, num_groups, eps)\u001b[0m\n\u001b[1;32m   3027\u001b[0m reduction_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m   3028\u001b[0m input_reshaped \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m   3029\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3030\u001b[0m     [batch_size, num_groups, num_channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, flattened_inner_size],\n\u001b[1;32m   3031\u001b[0m )\n\u001b[0;32m-> 3032\u001b[0m out, mean, rstd \u001b[38;5;241m=\u001b[39m \u001b[43m_normalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3033\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   3035\u001b[0m broadcast_dims \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mndim))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
