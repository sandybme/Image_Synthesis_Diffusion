{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sandhanakrishnan/anaconda3/envs/deepl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset\n",
    "from diffusers.optimization import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "# from torchinfo import summary\n",
    "\n",
    "from simple_diffusion.scheduler import DDIMScheduler\n",
    "from simple_diffusion.model import UNet\n",
    "from simple_diffusion.utils import save_images, normalize_to_neg_one_to_one\n",
    "from simple_diffusion.dataset import CustomDataset, get_dataset\n",
    "import pandas as pd\n",
    "import webdataset as wds\n",
    "\n",
    "from simple_diffusion.ema import EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = '/Users/Sandhanakrishnan/Image_Synthesis/Train'\n",
    "\n",
    "image_paths = [os.path.join(data_path, filename) for filename in os.listdir(data_path) if filename.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Create a DataFrame\n",
    "data_df = pd.DataFrame(image_paths, columns=['image_path'])\n",
    "csv_file_path = 'train.csv'\n",
    "data_df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "n_timesteps = 1000\n",
    "n_inference_timesteps = 250\n",
    "\n",
    "def _grayscale_to_rgb(img):\n",
    "    if img.mode != \"RGB\":\n",
    "        return img.convert(\"RGB\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"mps\")\n",
    "    model = UNet(3, image_size=args.resolution, hidden_dims=[64, 128, 256, 512])\n",
    "    noise_scheduler = DDIMScheduler(num_train_timesteps=n_timesteps,\n",
    "                                    beta_schedule=\"cosine\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    if args.pretrained_model_path:\n",
    "        pretrained = torch.load(args.pretrained_model_path)[\"model_state\"]\n",
    "        model.load_state_dict(pretrained)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "    )\n",
    "\n",
    "    tfms = transforms.Compose([\n",
    "        transforms.Resize((args.resolution, args.resolution)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    # df = pd.read_pickle(args.dataset_path)\n",
    "    df = pd.read_csv(args.dataset_path)\n",
    "    dataset = CustomDataset(df)\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "    steps_per_epcoch = len(train_dataloader)\n",
    "\n",
    "    total_num_steps = (steps_per_epcoch * args.num_epochs) // args.gradient_accumulation_steps\n",
    "    total_num_steps += int(total_num_steps * 10/100)\n",
    "    gamma = args.gamma\n",
    "    ema = EMA(model, gamma, total_num_steps)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps,\n",
    "        num_training_steps=total_num_steps,\n",
    "    )\n",
    "\n",
    "    # summary(model, [(1, 3, args.resolution, args.resolution), (1,)], verbose=1)\n",
    "\n",
    "    scaler = GradScaler(enabled=args.fp16_precision)\n",
    "    global_step = 0\n",
    "    losses = []\n",
    "    for epoch in range(args.num_epochs):\n",
    "        progress_bar = tqdm(total=steps_per_epcoch)\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        losses_log = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # print(batch[\"image\"])\n",
    "            clean_images = batch[\"image\"].to(device)\n",
    "            clean_images = normalize_to_neg_one_to_one(clean_images)\n",
    "\n",
    "            batch_size = clean_images.shape[0]\n",
    "            noise = torch.randn(clean_images.shape).to(device)\n",
    "            timesteps = torch.randint(0,\n",
    "                                      noise_scheduler.num_train_timesteps,\n",
    "                                      (batch_size,),\n",
    "                                      device=device).long()\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise,\n",
    "                                                     timesteps)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=args.fp16_precision):\n",
    "                noise_pred = model(noisy_images, timesteps)[\"sample\"]\n",
    "                loss = F.l1_loss(noise_pred, noise)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            ema.update_params(gamma)\n",
    "            gamma = ema.update_gamma(global_step)\n",
    "\n",
    "            if args.use_clip_grad:\n",
    "                clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            losses_log += loss.detach().item()\n",
    "            logs = {\n",
    "                \"loss_avg\": losses_log / (step + 1),\n",
    "                \"loss\": loss.detach().item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                \"step\": global_step,\n",
    "                \"gamma\": gamma\n",
    "            }\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "            # Generate sample images for visual inspection\n",
    "            if global_step % args.save_model_steps == 0:\n",
    "                ema.ema_model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # has to be instantiated every time, because of reproducibility\n",
    "                    generator = torch.manual_seed(0)\n",
    "                    generated_images = noise_scheduler.generate(\n",
    "                        ema.ema_model,\n",
    "                        num_inference_steps=n_inference_timesteps,\n",
    "                        generator=generator,\n",
    "                        eta=1.0,\n",
    "                        use_clipped_model_output=True,\n",
    "                        batch_size=args.eval_batch_size,\n",
    "                        output_type=\"numpy\")\n",
    "\n",
    "                    save_images(generated_images, epoch, args)\n",
    "\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            'model_state': model.state_dict(),\n",
    "                            'ema_model_state': ema.ema_model.state_dict(),\n",
    "                            'optimizer_state': optimizer.state_dict(),\n",
    "                        }, args.output_dir)\n",
    "\n",
    "        progress_bar.close()\n",
    "        losses.append(losses_log / (step + 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulationArgs:\n",
    "    def __init__(self,\n",
    "                 dataset_name='',\n",
    "                 dataset_path='/Users/Sandhanakrishnan/Image_Synthesis/train.csv',\n",
    "                 dataset_config_name=None,\n",
    "                 output_dir=\"trained_models/ddpm-model-64.pth\",\n",
    "                 samples_dir=\"test_samples/\",\n",
    "                 loss_logs_dir=\"training_logs\",\n",
    "                 cache_dir=None,\n",
    "                 resolution=64,\n",
    "                 train_batch_size=16,\n",
    "                 eval_batch_size=16,\n",
    "                 num_epochs=1,\n",
    "                 save_model_steps=1000,\n",
    "                 gradient_accumulation_steps=1,\n",
    "                 learning_rate=1e-4,\n",
    "                 lr_scheduler=\"cosine\",\n",
    "                 lr_warmup_steps=100,\n",
    "                 adam_beta1=0.9,\n",
    "                 adam_beta2=0.99,\n",
    "                 adam_weight_decay=0.0,\n",
    "                 use_clip_grad=False,\n",
    "                 logging_dir=\"logs\",\n",
    "                 pretrained_model_path=None,\n",
    "                 fp16_precision=False,\n",
    "                 gamma=0.996):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_config_name = dataset_config_name\n",
    "        self.output_dir = output_dir\n",
    "        self.samples_dir = samples_dir\n",
    "        self.loss_logs_dir = loss_logs_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.resolution = resolution\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.save_model_steps = save_model_steps\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.lr_warmup_steps = lr_warmup_steps\n",
    "        self.adam_beta1 = adam_beta1\n",
    "        self.adam_beta2 = adam_beta2\n",
    "        self.adam_weight_decay = adam_weight_decay\n",
    "        self.use_clip_grad = use_clip_grad\n",
    "        self.logging_dir = logging_dir\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.fp16_precision = fp16_precision\n",
    "        self.gamma = gamma\n",
    "\n",
    "    # Example validation method\n",
    "    def validate_args(self):\n",
    "        if self.dataset_name is None and self.dataset_path is None:\n",
    "            raise ValueError(\"You must specify either a dataset name or a dataset path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yfcc7m\n",
      "0.0002\n"
     ]
    }
   ],
   "source": [
    "args = SimulationArgs(dataset_name=\"yfcc7m\", train_batch_size=32, learning_rate=2e-4)\n",
    "\n",
    "# Access arguments like so\n",
    "print(args.dataset_name)\n",
    "print(args.learning_rate)\n",
    "\n",
    "# Don't forget to validate your arguments if needed\n",
    "args.validate_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
